# R programming

## R Review & Assessment

Let's take about an hour to take a quick assessment, touch up on basic R programming skills in a survey style, and then we'll move onto some challenges which should test your understanding if we have time.

### Learning Outcomes

- Assess where everyone is with R so we can shape the curricula
- Refresh general R programming skills
- Test R skills against some example problems

### Assessment

Everyone will arrive to this lesson with different experiences with R.
Skill with R doesn't necessarily exist a continuum and can instead be thought of as a set of tools.
Each particpant may start the workshop with different tools.

In order to get a better sense of what topics in R we should focus more heavily on, let's do a quick assessment.
The results will help us shape instruction so that we can ensure we're meeting everyone's needs.

**Instructions:** 

Answer the following 5 questions to the best of your knowledge and write down your answers.

#### Question 1

Which of the following expressions assigns the number 2 to the variable x?

Choose one or more:

- A. `x == 2`
- B. `x <- 2`
- C. `x - 2`
- D. `x = 2`

Your answer:

#### Question 2

What does the following expression return?

```{r}
paste("apple", "pie")
```

Choose one:

- A. "applepie"
- B. "apple, pie"
- C. "apple pie"
- D. An error

Your answer:

#### Question 3

What does the following expression return?

```{r}
max(abs(c(-5, 1, 5)))
```

Choose one:

- A. -5
- B. 1
- C. 5
- D. An error

Your answer:

#### Question 4

If x and y are both data.frames defined by:

```{r}
x <- data.frame(z = 1:2)
y <- data.frame(z = 3)
```

which of the following expressions would be a correct way to combine them into one data.frame that looks like this:

```
z
-
1
2
3
```

(i.e. one column with the numbers 1, 2, and 3 in it)

Choose one or more:

- A. `join(x, y)`
- B. `c(x, y)`
- C. `rbind(x, y)`
- D. `x + y`

Your answer:

#### Question 5
  
Given the following data.frame,

x <- data.frame(y = 1:10)

Which expression(s) return a `data.frame` with rows where y is greater than 5 (i.e. 6 - 10)

Choose one or more:

- A. `x[x$y > 5,]`
- B. ``x$y > 5`
- C. `x[which(x$y > 6),]`
- D. `x[y > 5,]`
- E. `subset(x, y > 5)`
    
### R overview

Based on previous R training: https://github.nceas.ucsb.edu/Training/R-intro.
Instructor will go over these live with the classroom, running 

- Basic syntax
  - Variables & assignemnt
  
#### The assignment operator, `<-`

One of the things we'll do all the time is save some value to a variable.
Here, we save the word "apple" to a variable called `fruit`

```{r}
fruit <- "apple"
fruit
```

Notice the last line with just `fruit` on it.
Typing just the variable name in just prints the value to the Console.

R has a flexible syntax.
The following two lines of code are identical to the above one.

```{r}
fruit<-"apple"
fruit    <-     "apple"
```

#### R as a calculator: `+ - * / > >= %% %/%` etc
  
```{r}
2+2
2 * 3
2 ^ 3
5/2
```

Comparison:

```{r}
2 == 1
2 == 2
3 > 2
2 < 3 # Same as above
"apple" == "apple"
"apple" == "pair"
"pair" == "apple" # Order doesn't matter for ==
```


#### Types of variables

##### Vectors

When we run a line of code like this:

```{r}
x <- 2
```

We're assigning 2 to a variable `x`.
`x` is a variable but it is also a "numeric vector" of length 1.

```{r}
class(x)
length(x)
```

Above, we ran two function: `class` and `length` on our variable `x`.
Running functions is a very common thing you'll do in R.
Every function has a name, following by a pair of `()` with something inside.

We can make a numeric vector that is longer like so:

```{r}
x <- c(1, 2, 3) # Use the `c` function to put things together
```

Notice we can also re-define a variable at a later point just like we did above.

```{r}
class(x)
length(x)
```

R can store much more than just numbers though.
Let's start with strings of characters, which we've already seen:

```{r}
fruit <- "apple"
class(fruit)
length(fruit)
```

Depending on your background, you may be surprised that the result of running `length(fruit)` is 1 because "apple" is five characters long.

It turns out that `fruit` is a character vector of length one, just like our numeric vector from before.
To find out the number of characters in "apple", we have to use another function:

```{r}
nchar(fruit)
nchar("apple")
```

Let's make a character vector of more than length one and take a look at how it works:

```{r}
fruits <- c("apple", "banana", "strawberry")
length(fruits)
nchar(fruits)
fruits[1]
```

Smushing character vectors together can be done with `paste`:

```{r}
paste("key", "lime", "pie")
```

##### Lists

Vectors and lists look similar in R sometimes but they have very different uses:

```{r}
c(1, "apple", 3)
list(1, "apple", 3)
```

##### data.frames
Most of the time when doing analysis in R you will be working with `data.frames`.
`data.frames` are tabular, with column headings and rows of data, just like a CSV file.

We create new `data.frames` with a relevantly-named function:

```{r}
mydata <- data.frame(site = c("A", "B", "C"),
                     temp = c(20, 30, 40))
mydata
```

Or we can read in a CSV from the file system and turn it into a `data.frame` in order to work with it in R:

```{r}
mydata <- read.csv("./data/data.csv")
mydata
```

We can find out how many rows of data `mydata` has in it:

```{r}
nrow(mydata)
```

We can return just one of the columns:

```{r}
mydata$type
unique(mydata$type)
```
sort

If we want to sort `mydata`, we use the `order` function (in kind of a weird way):

```{r}
mydata[order(mydata$type),]
```

Let's break the above command down a bit.
We can access the individual cells of a `data.frame` with a new syntax element: `[` and `[`:

```{r}
mydata[1,] # First row
mydata[,1] # First column
mydata[1,1] # First row, first column
mydata[c(1,5),] # First and second row
mydata$type # Column named 'type'
```

So what does that `order` function do?

```{r}
?order # How to get help in R!
order(c(1, 2, 3))
order(c(3, 2, 1))
order(mydata$type)
```

So `order(mydata$type)` is returning the rows of `mydata`, by row number, in sorted order.

We can also return just certain rows, based upon criteria:

```{r}
mydata[mydata$type == "fruit",]
mydata$type == "fruit"
```

In this case, instead of indexing the rows by number, we're using TRUEs and FALSEs.

**Exercise:** Subset `mydata` to the vegetables instead of the fruit

```{r}
# Your code here
```

Another handy way to subset `data.frame` is with the `subset` function:
    
```{r}
subset(mydata, type == "fruit") # Equivalent to mydata[mydata$type == "fruit",]
```

There are a lot of useful functions to help us work with `data.frame`s:

```{r}
str(mydata)
summary(mydata)
```

Our `data.frame`s won't always be so small as this example one.
Let's look at a larger one:

```{r}
library(ggplot2)
data("diamonds")
diamonds
```

**Exercise:** How many rows does diamonds have in it? How many columns?

We can look at the first few rows with `head`, just like on the command line:

```{r}
head(diamonds)
```

or the last few:

```{r}
tail(diamonds)
```
So far, this has probably been a bit boring.
Let's do something interesting and also something that R is very good at: Plotting and modeling!

Let's plot the relationship between diamond price and carat:

```{r}
plot(price ~ carat, data = diamonds)
```

The above syntax, `price ~ carat` uses a `response ~ predictor` form or `y ~ x`.

We can also fit a linear model to the same relationship:

```{r}
mod <- lm(price ~ carat, data = diamonds)
```

Above, we saved our linear model to a variable named `mod` so we can use it later.
We can look at the result of model fitting with `summary`.

```{r}
summary(mod)
```

And we can also plot the line of best fit on the scatterplot:

```{r}
plot(price ~ carat, data = diamonds)
abline(mod$coefficients[[1]], mod$coefficients[[2]], col = "red", lwd = 5)
```

### More advanced topics to cover if time allows:

- Modeling
  - `sample` / `runif` / `rnorm`
    - sampling a data.frame
- Common util functions
    - `table`
- Iteration
    - `for` loops
        - When do we use for loops?
    - `while` loops
- apply-family
  - `sapply`
  - `lapply`
- functions: We'll skip this as it will be taught in another lesson
- Writing faster code
  - What is 'vectorization'?
  - Don't grow variables
- Debugging
  - traceback()
  - browser()
  - breakpoints in RStudio
  
### Call your shot

An excellent way to really learn a programming language is to call out what the result of running some expression will be *before* you run it.
Afterwards, you can compare your expectation with what actually happened.

Here are code chunks with a series of expressions.
Try to predict what **the final expression* does before running the entire chunk and add a note if you got one wrong.

```{r}
x <- 2
x ^ 2
```

```{r}
x <- 1; y <- 2; x + y;
```

```{r}
x <- "hello"
y <- "world"
paste(x, y)
```

```{r}
x <- list(1, 2, 3)
y <- list(4, 5, 6)
z <- c(x, y)
length(z)
```

```{r}
x <- data.frame(x = 1:6)
y <- data.frame(x = 1:7)
z <- rbind(x, y)
nrow(z)
```


```{r}
x <- NA

if (is.na(x)) {
  print("foo")
} else {
  print("bar")
}
```

```{r}
numbers <- seq(1, 10)

for (number in numbers) {
  if (number %% 2){
    print(number)
  }
}
```

```{r}
x <- 10

while (x >= 0) {
  print(x)
  
  if (x == 5) {
    break
  }
  
  x <- x - 1
}
```

```{r}
x <- c(1, "2", 3)
class(x)
```

```{r}
x <- list(1, 2, 3)
lapply(x, cumsum)
```

```{r}
x <- data.frame(letter = LETTERS)
class(x[1,])
```

```{r}
x <- data.frame(letter = LETTERS)
class(x[1, 1, drop = FALSE])
```

```{r}
x <- data.frame(x = 1)
y <- data.frame(x = 2)
z <- rbind(x[1,1], y[1,1])
class(z)
```
```{r}
rep(TRUE, 5) & rep(FALSE, 5)
```

```{r}
rep(TRUE, 5) && rep(FALSE, 5)
```

```{r}
x <- c(1, 2, 3)
y <- c("A", "B", "C")
z <- c(x, y)
class(z)
```

```{r}
x <- c(1, 2, NA, 4, NA)
length(is.na(x))
```

```{r}
x <- c(1, NA, 3)
y <- c(NA, 2, NA)
all(is.na(x + y))
```

### Summary

By the end of this lesson, you should have feel touched up on your general R skills and you also should have seen some of the trickier parts of R.
Hopefully having seen the trickier parts of R will help later on down the road.

### Resources

Other good resources:

- One of our lessons on R: https://github.nceas.ucsb.edu/Training/R-intro
- Great, free book: http://r4ds.had.co.nz/
- DataCamp's R intro, https://www.datacamp.com/courses/free-introduction-to-r
Sources:
- Used https://www.rstudio.com/resources/cheatsheets/ to make sure I wasn't missing any basic stuff

<br>

## Creating R Functions

Many people write R code as a single, continuous stream of commands, often drawn
from the R Console itself and simply pasted into a script.  While any script 
brings benefits over non-scripted solutions, there are advantages to breaking
code into small, reusable modules.  This is the role of a `function` in R.  In
this lesson, we will review the advantages of coding with functions, practice 
by creating some functions and show how to call them, and then do some exercises
to build other simple functions.

### Leaning outcomes

- Learn why we should write code in small functions
- Write code for one or more functions
- Document functions to improve understanding and code communication

### Why functions?

In a word:

- DRY: Don't Repeat Yourself

By creating small functions that only one logical task and do it well, we quickly 
gain:

- Improved understanding
- Reuse via decomposing tasks into bite-sized chunks
- Improved error testing


### Temperature conversion

Imagine you have a bunch of data measured in Fahrenheit and you want to convert
that for analytical purposes to Celsius.  You might have an R script
that does this do you.

```{r}
airtemps <- c(212, 30.3, 78, 32)
celsius1 <- (airtemps[1]-32)*5/9
celsius2 <- (airtemps[2]-32)*5/9
celsius3 <- (airtemps[3]-32)*5/9
```

Note the duplicated code, where the same formula is repeated three times.  This
code would be both more compact and more reliable if we didn't repeat ourselves.

### Creating a function

Functions in R are a mechanism to process some input and return a value.  Similarly
to other variables, functions can be assigned to a variable so that they can be used
throughout code by reference.  To create a function in R, you use the `function` function (so meta!) and assign its result to a variable.  Let's create a function that calculates
celsius temperature outputs from fahrenheit temperature inputs.

```{r}
fahr_to_celsius <- function(fahr) {
  celsius <- (fahr-32)*5/9
  return(celsius)
}
```

By running this code, we have created a function and stored it in R's global environment.  The `fahr` argument to the `function` function indicates that the function we are creating takes a single parameter (the temperature in fahrenheit), and the `return` statement indicates that the function should return the value in the `celsius` variable that was calculated inside the function.  Let's use it, and check if we got the same value as before:

```{r}
celsius4 <- fahr_to_celsius(airtemps[1])
celsius4
celsius1 == celsius4
```

Excellent.  So now we have a conversion function we can use.  Note that, because 
most operations in R can take multiple types as inputs, we can also pass the original vector of `airtemps`, and calculate all of the results at once:

```{r}
celsius <- fahr_to_celsius(airtemps)
celsius
```

This takes a vector of temperatures in fahrenheit, and returns a vector of temperatures in celsius.

### Exercise

Now, create a function named `celsius_to_fahr` that does the reverse, it takes temperature data in celsius as input, and returns the data converted to fahrenheit.  Then use that formula to convert the `celsius` vector back into a vector of fahrenheit values, and compare it to the original `airtemps` vector to ensure that your answers are correct.

```{r}
# Your code goes here
```

Did you encounter any issues with rounding or precision?

### Documenting R functions

Functions need documentation so that we can communicate what they do, and why.  The `roxygen2` package provides a simple means to document your functions so that you can explain what the function does, the assumptions about the input values, a description of the value that is returned, and the rationale for decisions made about implementation.

Documentation in ROxygen is placed immediately before the function definition, and is indicated by a special comment line that always starts with the characters `#'`.  Here's a documented version of a function:

```{r}
#' Convert temperature data from Fahrenheit to Celsius
#'
#' @param fahr Temperature data in degrees Fahrenheit to be converted
#' @return temperature value in degrees Celsius
#' @keywords conversion
#' @export
#' @examples
#' fahr_to_celsius(32)
#' fahr_to_celsius(c(32, 212, 72))
fahr_to_celsius <- function(fahr) {
  celsius <- (fahr-32)*5/9
  return(celsius)
}
```

Note the use of the `@param` keyword to define the expectations of input data, and the `@return` keyword for defining the value that is returned from the function.  The `@examples` function is useful as a reminder as to how to use the function.  Finally, the `@export` keyword indicates that, if this function were added to a package, then the function should be available to other code and packages to utilize.

### Summary

- Functions are useful to reduce redundancy, reuse code, and reduce errors
- Build functions with the `function` function
- Document functions with `roxygen2` comments

<br>

## Creating R packages

### Why packages?

Most R users are familiar with loading and utilizing packages in their work. And they know how rich CRAN is in providing for many conceivable needs.  Ironically, most have never created a package for their own work, and most think the process is too complicated.  It is not.  Creating packages serves two main use cases:

- Mechanism to redistribute reusable code
- Mechanism to reproducibly document analysis and models and their results

The `devtools` and `roxygen` package makes creating and maintaining a package to be a straightforward experience.

Creating a package for your own personal use is a convenient way to build utility functions that you can include throughout your code, rather than using `source` to import files.  Using `source` can work, but it makes your code more fragile by introducing dependencies on the specific locations of the files that you source.  In comparison, an installed package is available to your code anywhere, and is easily updatable.

### Install and load packages

```{r, eval=FALSE}
# install.packages("devtools")
library(devtools)
library(roxygen2)
```

### Create a directory for the package

Thanks to the great [devtools](https://github.com/hadley/devtools) package, it only takes one function call to create the skeleton of an R package using `create()`.  Which eliminates pretty much all reasons for procrastination.  All you do is:

```{r eval=FALSE}
library("devtools")
create("mytools")
```

This will create a top-level directory structure, including a number of critical files under the [standard R package structure](http://cran.r-project.org/doc/manuals/r-release/R-exts.html#Package-structure).  The most important of which is the `DESCRIPTION` file, which provides metadata about your package. Edit the `DESCRIPTION` file to provide reasonable values for each of the fields. Information about choosing a LICENSE is provided in the [Extending R](http://cran.r-project.org/doc/manuals/r-release/R-exts.html#Licensing) documentation.

```{r eval=FALSE}
Package: mytools
Title: Utility functions created by Matt Jones
Version: 0.1
Authors@R: "Matthew Jones <jones@nceas.ucsb.edu> [aut, cre]"
Description: Package mytools contains a suite of utility functions useful whenever I need stuff to get done.
Depends: R (>= 3.1.0)
License: Apache License (== 2.0)
LazyData: true
```

For discussion on when to list a package under Imports or Depends, see [this discussion on StackOverflow](http://stackoverflow.com/questions/8637993/better-explanation-of-when-to-use-imports-depends). But in brief:

Avoid depends as much as possible. It's basically like saying `library(other_package)` every time your package is loaded. This can lead to a large number of dependencies being installed each time. Instead, list a package under Imports like so:

```
Imports:
    ggplot2
```

The in the documentation for any package, add the following line:

```
#' @importFrom ggplot2 ggplot
```
This will ensure that only the necessary functions and packages are downloaded and specific functions are referenced by NAMESPACE. If you need to use all (or 
most functions from a package), you can import all of its functions:

```
#' @import ggplot2
```

For an example to poke around, see the `codyn` package that came out of the NCEAS community dynamics working group [https://github.com/laurenmh/codyn](https://github.com/laurenmh/codyn)

See how we listed an import: 
1. https://github.com/laurenmh/codyn/blob/master/DESCRIPTION#L24
2. and a list of specifically imported functions from a package: https://github.com/laurenmh/codyn/blob/master/R/community_stability.R#L28

---

### Add your code

The skeleton package created contains a directory `R` which should contain your source files.  Add your functions and classes in files to this directory, attempting to choose names that don't conflict with existing packages.  For example, you might add a file `info.R` that contains a function `environment_info()` that you might want to reuse. This one might leave something to be desired...

```{r}
environment_info <- function(msg) {
    print("This should really do something useful!")
    print(paste("Also print the incoming message: ", msg))
}
```

### Add documentation

You should provide documentation for each of your functions and classes.  This is done in the `roxygen2` approach of providing embedded comments in the source code files, which are in turn converted into manual pages and other R documentation artifacts.    Be sure to define the overall purpose of the function, and each of its parameters.

```{r}
#' A function to print information about the current environment.
#'
#' This function prints current environment information, and a message.
#' @param msg The message that should be printed
#' @keywords debugging
#' @export
#' @examples
#' environment_info("Hi, what is your name?")
environment_info <- function(msg) {
    print("This should really do something useful!")
    print(paste("Also print the incoming message: ", msg))
}
```

Once your files are documented, you can then process the documentation using the `document()` function to generate the appropriate .Rd files that your package needs.

```{r eval=FALSE}
setwd("./mytools")
document()
# or document(".")
```

That's really it.  You now have a package that you can `check()` and `install()` and `release()`.  See below for these helper utilities.

### Exercise

Add two functions to the `mytools` package for converting temperatures from Fahrenheit to Celsius and back.  Then check, build, and install the package, and then use it from the console to do some conversions.

### Checking and installing your package

Now that your package is built, you can check it for consistency and completeness using `check()`, and then you can install it locally using `install()`, which needs to be run from the parent directory of your module.

```{r eval = FALSE}
setwd("./mytools")
check()
setwd("..")
install("mytools")
```

Your package is now available for use in your local environment.

### Sharing and releasing your package

The simplest way to share your package with others is to upload it to a [GitHub repository](https://github.com), which allows others to install your package using the `install_github()` function from `devtools`.

If your package might be broadly useful, also consider releasing it to CRAN, using the `release()` method from `devtools().

<br>


## Testing R Code and Analysis

When we write R code to do our analyses, we're usually making use of other peoples' functions.
But sometimes we have a need to write our own functions and, when we do this, it is a great idea to test them to make sure they work as we expect.

When writing functions, we often develop them against some set of test data.

```{r}
input_values <- c(1, 2, 3)
my_log_transform <- function(values) {
  log(values)
}
my_log_transform(input_values) # Should be: 0.0000000 0.6931472 1.0986123
```
We do this with data we know the result of running our function on so we can verify that our code actually works.
In reality, we often find we didn't test with enough different types of test data to cover data our users might pass to our function.

```{r}
log(-1)
```
This produces a weird result:

> NaNs produced
> [1] NaN

Or maybe our user tries something more unexpected:

```{r, eval=FALSE}
my_log_transform("bananas")
```

This results in an error:

> Error in log(values) : non-numeric argument to mathematical function

Testing helps us make sure our code produces a correct result.
But it also helps us get in the mindest of our users:

- What values might we expect a user to pass in. e.g. does our function accept numbers *and* strings or just numbers?
- What behavior might the user expect when certain values are pased in: Does our function produce a cryptic or useful error message when something goes wrong?

Creating a useful function is both about writing a correct function but also a function that is easy to use.

### Lesson

As an example of how to test, we'll create a new R package that provides a single function to calculate Z scores for a vector of numbers.
A Z score tells us how many standard deviations a value is from the mean of a set of values.
Mathematically, it is defined as $z = (X - μ) / σ$ where $X$ is the value, $µ$ is the mean and $σ$ is the standard deviation.
In reality, this is already proivded by R with the `scale` function but we're going to write it for ourselves as a simple example.

### Task one: Create a new R package in RStudio

There are two packages we'll need to get set up testing:

- testthat
- devtools

If you need then, install them with:

```{r, eval=FALSE}
install.packages(c("testthat", "devtools"))
```

`testthat` is the easiest way to get started testing your R code.
It uses a convention to help you organize your tests and help itself find and run them.
Your tests live in one or more files that start with the filename `test_` (e.g. `test_z_score.R`, `test_analysis.R`) inside the `tests/testthat/` folder in the root of your package:

```
.
├── DESCRIPTION
├── NAMESPACE
├── R
│   └── hello.R
├── man
│   └── hello.Rd
├── testingdemo.Rproj
└── tests
    ├── testthat
    │   └── test files go here!
    └── testthat.R
```

Let's get started using `testthat` with:

```{r, eval=FALSE}
devtools::use_testthat()
```

This does a few things to our package:

- Adds `testthat` to the Suggests section of the DESCRIPTION file
- Adds the `tests` folder at the root of our package with a place to put our tests

Let's get started:

- Make a new R script at `R/z_score.R`
- Fill it with or type in:

```{r}
z_score <- function(x) {
  (x - mean(x)) / sd(x)
}
```

Before we get to testing our new `z_score` function, make sure to see how it works.
Go ahead and "source" `R/z_score.R` and try it out a few times to see.
For example, you could try `z_score(c(1, 2, 3))`.

Let's write our first test for `z_score`:

First:

- Create an empty R script at `tests/testthat/test_z_score.R`

`testthat` tests take are all function calls to a function called `test_that()`.
Each test file will just be a number of these `test_that()` functions being called.
There are no real restrictions on how you test your code but, typically, people tend to make each call to `test_that` test some specific behavior:

For example:

- Less good: 
    - "Test that z_scale works" (this is vague)
- Better:
    - "Test that z_scale returns correct values"
    - "Test that z_scale produces an error when passed non-sensical input"

A simple function may have only a test or two, depending on how simple it is.
As your functions get more complex, you may need to write many tests, so keeping tests specific can help your organize your tests.

They look like this:

```r
test_that("z_scale returns correct values", {
  expect_identical(z_scale(c(1, 2, 3)), as.numeric(t(scale(c(1,2,3)))))
})
```

Let's break it down and talk about each part.

Each `test_that()` call has two arguments: A description (character) and the code for the test, wrapped in curly braces.

```r
test_that("z_scale returns correct values", { # your code here })
```

And the code can be any R code you want, but should contain at least one call to an expectation function, like `expect_true`.
`testthat` comes with a number of `expect_` functions like:

- `expect_equal`
- `expect_true`
- `expect_length`
- `expect_error` (checks if the code produces an error)

<br>

## Quick Intro to Parallel Computing in R

### Learning Outcomes

- Understand what parallel computing is and when it may be useful
- Understand how parallelism can work
- Review sequential loops and *apply functions
- Understand and use the `parallel` package multicore functions
- Understand and use the `foreach` package functions

### Introduction

Processing large amounts of data with complex models can be time consuming.  New types of sensing means the scale of data collection today is massive. And modeled outputs can be large as well.For example, here's a 2 TB (that's Terabyte) set of modeled output data from Ofir Levy et al. 2016 that models 15 environmental variables at hourly time scales for hundreds of years across a regular grid spanning a good chunk of North America:

![Levy et al. 2016. doi:10.5063/F1Z899CZ](images/levy-map.png)

There are over 400,000 individual netCDF files in the [Levy et al. microclimate data set](https://doi.org/10.5063/F1Z899CZ).  Processing them would benefit massively from parallelization.

Alternatively, think of remote sensing data.  Processing airborne hyperspectral data can involve processing each of hundreds of bands of data for each image in a flight path that is repeated many times over months and years.  

![NEON Data Cube](images/DataCube.png)


### Why parallelism?

Much R code runs fast and fine on a single processor.  But at times, computations
can be:

- **cpu-bound**: Take too much cpu time
- **memory-bound**: Take too much memory
- **I/O-bound**: Take too much time to read/write from disk
- **network-bound**: Take too much time to transfer

To help with **cpu-bound** computations, one can take advantage of modern processor architectures that provide multiple cores on a single processor, and thereby enable multiple computations to take place at the same time. In addition, some machines ship with multiple processors, allowing large computations to occur across the entire cluster of those computers. Plus, these machines also have large amounts of memory to avoid **memory-bound** computing jobs.

### Processors (CPUs) and Cores

A modern CPU (Central Processing Unit) is at the heart of every computer.  While
traditional computers had a single CPU, modern computers can ship with mutliple processors, which in turn can each contain multiple cores.  These processors and 
cores are available to perform computations.

A computer with one processor may still have 4 cores (quad-core), allowing 4 computations
to be executed at the same time.

![](images/processor.png) 

A typical modern computer has multiple cores, ranging from one or two in laptops
to thousands in high performance compute clusters.  Here we show four quad-core
processors for a total of 16 cores in this machine.

![](images/processors.png)

You can think of this as allowing 16 computations to happen at the same time. Theroetically, your computation would take 1/16 of the time (but only theoretically, more on that later).

Historically, R has only utilized one processor, which makes it single-threaded.  Which is a shame, because the 2017 MacBook Pro that I am writing this on is much more powerful than that:

```{bash eval=FALSE}
jones@powder:~$ sysctl hw.ncpu hw.physicalcpu
hw.ncpu: 8
hw.physicalcpu: 4
```

To interpret that output, this machine `powder` has 4 physical CPUs, each of which has
two processing cores, for a total of 8 cores for computation.  I'd sure like my R computations to use all of that processing power.  Because its all on one machine, we can easily use *multicore* processing tools to make use of those cores. Now let's look at
the computational server `aurora` at NCEAS:

```{bash eval=FALSE}
jones@aurora:~$ lscpu | egrep 'CPU\(s\)|per core|per socket'
CPU(s):                88
On-line CPU(s) list:   0-87
Thread(s) per core:    2
Core(s) per socket:    22
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87
```

Now that's some compute power!  Aurora has 384 GB of RAM, and ample storage. All still under the control of a single operating system. 

However, maybe one of these NSF-sponsored high performance computing clusters (HPC) is looking attractive about now:

- [JetStream](https://jetstream-cloud.org/)
    - 640 nodes, 15,360 cores, 80TB RAM
- [Stampede2]() at TACC is coming online in 2017
    - 4200 nodes, 285,600 cores

Note that these clusters have multiple nodes (hosts), and each host has multiple cores. So this is really multiple computers clustered together to act in a coordinated fashion, but each node runs its own copy of the operating system, and is in many ways independent of the other nodes in the cluster.  One way to use such a cluster would be to use just one of the nodes, and use a multi-core approach to parallelization to use all of the cores on that single machine.  But to truly make use of the whole cluster, one must use parallelization tools that let us spread out our computations across multiple host nodes in the cluster.

### When to parallelize

It's not as simple as it may seem. While in theory each added processor would linearly increase the throughput of a computation, there is overhead that reduces that efficiency.  For example, the code and, importantly, the data need to be copied to each additional CPU, and this takes time and bandwidth.  Plus, new processes and/or threads need to be created by the operating system, which also takes time.  This overhead reduces the efficiency enough that realistic performance gains are much less than theoretical, and usually do not scale linearly as a function of processing power. For example, if the time that a computation takes is short, then the overhead of setting up these additional resources may actually overwhelm any advantages of the additional processing power, and the computation could potentially take longer!

In addition, not all of a task can be parallelized. Depending on the proportion, the expected speedup can be significantly reduced.  Some propose that this may follow [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law), where the speedup of the computation (y-axis) is a function of both the number of cores (x-axis) and the proportion of the computation that can be parallelized (see colored lines):

```{r label="amdahl", echo=FALSE}
library(ggplot2)
library(tidyr)
amdahl <- function(p, s) {
  return(1 / ( (1-p) + p/s  ))
}
doubles <- 2^(seq(0,16))
cpu_perf <- cbind(cpus = doubles, p50 = amdahl(.5, doubles))
cpu_perf <- cbind(cpu_perf, p75 = amdahl(.75, doubles))
cpu_perf <- cbind(cpu_perf, p85 = amdahl(.85, doubles))
cpu_perf <- cbind(cpu_perf, p90 = amdahl(.90, doubles))
cpu_perf <- cbind(cpu_perf, p95 = amdahl(.95, doubles))
#cpu_perf <- cbind(cpu_perf, p99 = amdahl(.99, doubles))
cpu_perf <- as.data.frame(cpu_perf)
cpu_perf <- cpu_perf %>% gather(prop, speedup, -cpus)
ggplot(cpu_perf, aes(cpus, speedup, color=prop)) + 
  geom_line() +
  scale_x_continuous(trans='log2') +
  theme_bw() +
  labs(title = "Amdahl's Law")
```


So, its important to evaluate the computational efficiency of requests, and work to ensure that additional compute resources brought to bear will pay off in terms of increased work being done.  With that, let's do some parallel computing...

### Loops and repetitive tasks using lapply

When you have a list of repetitive tasks, you may be able to speed it up by adding more computing power.  If each task is completely independent of the others, then it is a prime candidate for executing those tasks in parallel, each on its own core.  For example, let's build a simple loop that uses sample with replacement to do a bootstrap analysis.  In this case, we select `Sepal.Length` and `Species` from the `iris` dataset, subset it to 100 observations, and then iterate across 10,000 trials, each time resampling the observations with replacement.  We then run a logistic regression fitting species as a function of length, and record the coefficients for each trial to be returned. 

```{r label="bootstrap-loop"}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 10000
res <- data.frame()
system.time({
  trial <- 1
  while(trial <= trials) {
    ind <- sample(100, 100, replace=TRUE)
    result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
    r <- coefficients(result1)
    res <- rbind(res, r)
    trial <- trial + 1
  }
})
```

The issue with this loop is that we execute each trial sequentially, which means that only one of our 8 processors on this machine are in use.  In order to exploit parallelism, we need to be able to dispatch our tasks as functions, with one task
going to each processor.  To do that, we need to convert our task to a function, and then use the `*apply()` family of R functions to apply that function to all of the members of a set.  In R, using `apply` is often significantly faster than the equivalent code in a loop.  Here's the same code rewritten to use `lapply()`, which applies a function to each of the members of a list (in this case the trials we want to run):

```{r label="bootstrap-lapply"}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- seq(1, 10000)
boot_fx <- function(trial) {
  ind <- sample(100, 100, replace=TRUE)
  result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
  r <- coefficients(result1)
  res <- rbind(data.frame(), r)
}
system.time({
  results <- lapply(trials, boot_fx)
})
```

### Approaches to parallelization
When parallelizing jobs, one can:

- Use the multiple cores on a local computer through `mclapply`
- Use multiple processors on local (and remote) machines using `makeCluster` and `clusterApply`

    - In this approach, one has to manually copy data and code to each cluster member using `clusterExport`
    - This is extra work, but sometimes gaining access to a large cluster is worth it
  
### Parallelize using: mclapply

The `parallel` library can be used to send tasks (encoded as function calls) to each of the processing cores on your machine in parallel.  This is done by using the `parallel::mclapply` function, which is analogous to `lapply`, but distributes the tasks to multiple processors. `mclapply` gathers up the responses from each of these function calls, and returns a list of responses that is the same length as the list or vector of input data (one return per input item).

```{r label="kmeans-comparison"}
library(parallel)
library(MASS)

starts <- rep(100, 40)
fx <- function(nstart) kmeans(Boston, 4, nstart=nstart)
numCores <- detectCores()
numCores

system.time(
  results <- lapply(starts, fx)
)

system.time(
  results <- mclapply(starts, fx, mc.cores = numCores)
)
```

Now let's demonstrate with our bootstrap example:
```{r label="bootstrap-mclapply"}
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- seq(1, 10000)
boot_fx <- function(trial) {
  ind <- sample(100, 100, replace=TRUE)
  result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
  r <- coefficients(result1)
  res <- rbind(data.frame(), r)
}
system.time({
  results <- mclapply(trials, boot_fx, mc.cores = numCores)
})
```

### Parallelize using: foreach and doParallel

The normal `for` loop in R looks like:

```{r label="for-loop"}
for (i in 1:3) {
  print(sqrt(i))
}
```

The `foreach` method is similar, but uses the sequential `%do%` operator to indicate an expression to run. Note the difference in the returned data structure. 
```{r label="foreach-loop"}
library(foreach)
foreach (i=1:3) %do% {
  sqrt(i)
}
```

In addition, `foreach` supports a parallelizable operator `%dopar%` from the `doParallel` package. This allows each iteration through the loop to use different cores or different machines in a cluster.  Here, we demonstrate with using all the cores on the current machine:
```{r label="foreach-doParallel"}
library(foreach)
library(doParallel)
registerDoParallel(numCores)  # use multicore, set to the number of our cores
foreach (i=1:3) %dopar% {
  sqrt(i)
}

# To simplify output, foreach has the .combine parameter that can simplify return values

# Return a vector
foreach (i=1:3, .combine=c) %dopar% {
  sqrt(i)
}

# Return a data frame
foreach (i=1:3, .combine=rbind) %dopar% {
  sqrt(i)
}
```

The [doParallel vignette](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf) on CRAN shows a much more realistic example, where one can use `%dopar% to parallelize a bootstrap analysis where a data set is resampled 10,000 times and the analysis is rerun on each sample, and then the results combined:

```{r label="foreach-bootstrap"}
# Let's use the iris data set to do a parallel bootstrap
# From the doParallel vignette, but slightly modified
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- 10000
system.time({
  r <- foreach(icount(trials), .combine=rbind) %dopar% {
    ind <- sample(100, 100, replace=TRUE)
    result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
    coefficients(result1)
  }
})

# And compare that to what it takes to do the same analysis in serial
system.time({
  r <- foreach(icount(trials), .combine=rbind) %do% {
    ind <- sample(100, 100, replace=TRUE)
    result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
    coefficients(result1)
  }
})

# When you're done, clean up the cluster
stopImplicitCluster()
```


### Summary

In this lesson, we showed examples of computing tasks that are likely limited by the number of CPU cores that can be applied, and we reviewed the architecture of computers to understand the relationship between CPU processors and cores.  Next, we reviewed the way in which traditional `for` loops in R can be rewritten as functions that are applied to a list serially using `lapply`, and then how the `parallel` package `mclapply` function can be substituted in order to utilize multiple cores on the local computer to speed up computations. Finally, we installed and reviewed the use of the `foreach` package with the `%dopar` operator to accomplish a similar parallelization using multiple cores.

### Readings and tutorials

- [Multicore Data Science with R and Python](https://blog.dominodatalab.com/multicore-data-science-r-python/)
- [Beyond Single-Core R](https://ljdursi.github.io/beyond-single-core-R/#/) by Jonoathan Dursi (also see [GitHub repo for slide source](https://github.com/ljdursi/beyond-single-core-R))
- The venerable [Parallel R](http://shop.oreilly.com/product/0636920021421.do) by McCallum and Weston (a bit dated on the tooling, but conceptually solid)
- The [doParallel Vignette](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf)




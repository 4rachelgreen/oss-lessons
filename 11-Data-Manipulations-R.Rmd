# Data Manipulations in R

## tidyverse: wrangling and surfing data with dplyr and friends

![](./images/datasurfing.jpg)  
<br>

[Data wrangling slide deck](http://www.slideshare.net/cjlortie/data-wrangling-in-r-be-a-wrangler)

### Introduction
<b> Philosophy of tidyverse </b>
Tidy data make your life easier. Data strutures should match intuition and common sense. Data should have logical structure.  Rows are are observations, columns are variables. Tidy data also increase the viability that others can use your data, do better science, reuse science, and help you and your ideas survive and thrive. A workflow should also include the wrangling you did to get your data ready. If data are already very clean in a spreadsheet, they can easily become a literate, logical dataframe. Nonetheless, you should still use annotation within the introductory code to explain the meta-data of your data to some extent and what you did pre-R to get it tidy.  The philosophy here is very similar to the data viz lesson forthcoming with two dominant paradigms. Base R code functions, and pipes %>% and the logic embodied within the libraries associated with the the tidyverse. Generally, I prefer the tidyverse because it is more logical and much easier to remember.  It also has some specific functions needed to address common errors in modern data structures with little code needed to fix them up.

[Why to love the tidverse](https://www.slideshare.net/cjlortie/whyr)  


### Learning outcomes
1. To appreciate the differences between base R and tidyverse packages in terms of function and grammmar.  
2. To be able to use dplyr functions to wrangle data and solves common challenges in datasets.
3. To be able to check and address missing values.
4. To be able to grab part of a dataset.
5. To use pipes to move/wrangle chunks of your dataset.

### Key concepts listed

<b>Data wrangling</b>

<b>Base R</b>
key concepts:
aggregate
tapply
sapply 
lappy
subsetting
as.factor
is.numeric
na

<b>tidyverse</b>
key concepts:
pipes are you best friend!
%>% 

dplyr
filter for rows
select for columns
mutate for new variables
summarise for bringing together many values
 
### Additional resources

[Excellent list of wrangling tools](http://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html)


[Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

[tidyr](https://cran.r-project.org/web/packages/tidyr/README.html)

[Great wrangling webinar](https://www.rstudio.com/resources/webinars/data-wrangling-with-r-and-rstudio/)

Here are some exercises to address basic challenges you will certainly encounter.  

### Exercise 1. Missing data

Missing data are typically termed NAs. Missing data are not necessarily the same as true zeros. See [Mixed Effects Models and Extension in Ecology with R](https://link.springer.com/content/pdf/10.1007%2F978-0-387-87458-6.pdf) for an excellent discussion of this. Missing data due to experimental design, observer error, or failures to detect a process are 'false zeros' and equivalent to missing data. True negatives (measured and detected absences) are recommended to be coded as a zero.

The read_csv from the tidyverse handles data import well, does not assume factors, and typically takes a parsimonous approach to vector interpretation. The analog from base R is read.csv but comes with many limitations.

```{r, missing data}
#Missing data. In error, missing cells/observations in some measure can kick back an error. In other apps, sometimes ignored but can introduce error.
#setwd('data-wrangling')
library(tidyverse)
ttc <-read_csv("./data/ttc.csv") #The Toronto Transit Commission Data.
ttc #tibble produced if use read_csv versus dataframe. Tibbles include str() list of vector attributes at the top of output.

#check for missing values
is.na(ttc) #returns a logical vector, true is missing (i.e. NA), false is present
summary(ttc, na.rm=TRUE) #excludes NA
new.ttc <-na.omit(ttc) # returns without missing values
is.na(new.ttc) #check to see if it worked
setdiff(ttc, new.ttc)

#many other solutions but I use these two frequently

```

### Exercise 2. Selecting part of a dataset

Dplyr provides select (columns/vectors), filter (rows/observations), and now pull (returns a vector not df or tibble). Fantastic functions (and beasts) to wrangle out specifics or simplify your dataframe.

```{r, selections}
survey<-read_csv("./data/5081.survey.1.csv")
survey
#I want just a simple tibble with experience by discipline for students new to rstats in a graduate-level course.

experience <- survey %>% dplyr::select(discipline, r.experience)
experience

#Now I just want to select the physiology folks
physiologists <- experience %>% filter(discipline == "physiology")
physiologists

#Selections also often include a summary by levels or I want to make a new column with some calculations. Think about what you have likely done in excel.

#used pipes and made a nice summary table
experience <-survey %>% group_by(discipline) %>% summarise(
  count = n(),
  exp = mean (r.experience)
)

```

### Exercise 3. Updated dplyr package

To explore the Star Wars dataset within the tidyverse updated package dplyr 0.7.0 and make a new column.
Full details list on its own [blog](https://blog.rstudio.org/2017/06/13/dplyr-0-7-0/).

<br>

![](./images/starwars.png)
<br>

```{r loads, warning=FALSE, message=FALSE}
library(tidyverse)
data <- starwars #setup as tibble
data #quick look

#simplify species
data <- data %>% mutate(taxa = ifelse(species == "Droid", "machine", "biological"))
data <- data[!is.na(data$taxa), ] #remove NAs from one vector only

#note, ifelses are messy and easy to get really nested and long-winded. Dplyr 0.7.0 release notes suggests considering the function case_when in its stead.

#count up by gender
counts <- data %>% group_by(gender) %>% count()
counts <- counts[!is.na(counts$gender), ] #remove NAs from one vector only
counts$gender <- factor(counts$gender, levels = counts$gender[order(counts$n)])

#I love the count and tally functions! There is also now a new set of similar functions: add_count() and add_tally() for adding an n column within groups

```

<br>

**Data viz**
```{r viz, warning=FALSE, message=FALSE}
#library(plotly)
p <- ggplot(data, aes(height, mass, color = taxa)) + geom_point(size = 3, alpha = 0.5) +
  scale_color_brewer(palette = "Dark2")
#ggplotly(p)
p


p <- ggplot(counts, aes(gender, n)) + geom_bar(stat = "identity", fill = "forestgreen", width = 0.25) + coord_flip()
p

```

<br>

### Exercise 4. Two-table wrangle challenge

Data are likely to be distributed amongst different tables and files. The updated package dpyr (and tidy friend packages) provides an excellent set of verbs to address this challenge.

Take a moment and review the brief blog post [here](http://dplyr.tidyverse.org/articles/two-table.html) describing the two categories of tools. Now, on your own, examine two datasets using the three families of tools (mutating, filtering joins, and set element exploration). Select ONE of the following challenges.

**SEAMAP Challenge**
```{r SEAMAP, warning=FALSE, message=FALSE}

seamap <- read_csv("./data/BGSREC.csv") #all meta-data listed in the PDF in directory
seamap

cruises <- read_csv("./data/CRUISES.csv")
cruises

```


**Whole-brain Challenge**
```{r brains, warning=FALSE, message=FALSE}

brains <- read_csv("./data/brains.csv")
brains

ninja.scores <- read_csv("./data/ninja.scores.csv")
ninja.scores

```

<br>  

### Interpretation of the tidyverse  

1. Even a simple datframe can provide adequate complexity for padawan and jedi alike.  
2. Literate coding is embodied with the tidyverse structure.  
3. The code is not verbose but gets the job done.  
4. Just a few keystrokes and a nice code snippet can allow you to surf an ocean of data. The code is not limited by the depth or width of the data. The term wrangling often implies rounding-up individuals into groups for management but really the power of dplyr and the tidyverse is in providing coherent, clear code grammar that works with almost almost all data (distributed, deep, and diverse) provided the structure can be tidied.

![](./images/padawans.jpg)  



## Data Munging, QA / QC and Cleaning
  
### Learning outcomes
  
Students should

- Learn what it means to test assumptions about their data with base R
- Learn how to reshape and restructure their data (tidyr)
- Be introduced to the `assertr` package for analysis QA

### Lesson

Let's look at an example dataset (one I've made) and go over some ways we might QA/QC this dataset.

It is an arbitrary temperature study with six sites. Within each site, there are 6 plots and we took 10 samples of temperature (degrees C) per plot.

#### Step one: Import and prepare

```{r}
fundata <- read.csv("./data/fundata.csv", stringsAsFactors = FALSE)
head(fundata) # Look at the data
str(fundata)
summary(fundata)
```

What's our assessment of this dataset from the above commands?

- The site and plot codes are smushed together in one column
- The dates look a bit funky. What format is that?
- The range on the temperatures surely can't be right

##### Split siteplot into two columns

```{r}
library(tidyr)

lettersdf <- data.frame(letters = paste(LETTERS, rev(LETTERS), sep = "."))
lettersdf
lettersdf %>%
  separate(letters, c("letter_one", "letter_two"), sep = "\\.")
```


**Exercise:** Split the `siteplot` column into two columns called `site` and `plot`:
  
```{r}
# Your code here
```

##### Convert the dates to real R dates

When you have dates in R, it's usually best to convert them to a `Date` object.
This is mainly so plotting functions work correctly but it helps elsewhere too.

The function we'll use is `as.Date`:
  
```{r}
datestrings <- c("2000-08-03", "2017-02-20", "1980-04-27")
mydate <- as.Date(datestrings, format = "%Y-%m-%d")
mydate
class(mydate)
```

** Exercise: Convert the `date` column in `fundata` to from a character vector to a Date vector with `as.Date()`:
  
```{r}
# Your code here
# e.g fundata$date <- 
```

#### Step two: Checking assumptions in the data

##### site & plot

Let's start by looking at the `site` column for potential issues.

The `table` function is a great way tally the occurrence of each of the unique values in a vector:

```{r}
table(c(5, 5, 2, 2, 3, 3))
table(c("A", "A", "B", "C")) # Also good for character vectors

fish <- c("gag grouper", "striped bass", "red drum", "gag grouper")
table(fish)
```

```{r}
table(fundata$site)
```

From the summary of the dataset, we would expect 60s across the board there.
But we don't see that!
  
  We can see we're missing some observations from A, C, and E.
Depending on our needs, we may need to go back to our field notes to find out what happened.

##### temp_c

Look with `summary`:

```{r}
summary(fundata)
```

Look directly with `range`:

```{r}
range(fundata$temp_c)
```

**Exercise:** Can we use an exploratory plot to check this assumption too?

Plotting your data is always a good idea and we can often find new insights or issues this way.
For univariate data, a box plot is a great way to look at the distribution of values:

```{r}
boxplot(rnorm(100, 50, 5))
```

Use `boxplot` to make a boxplot of the temperature values:

```{r}
# Your code here
```

Before we move on, let's fix the -9999 and 180 observations by removing them:
  
**Exercise: Remove the rows with temperatures of -9999 and 180:**
  
```{r}
# Your code here
```

```{r}
any(is.na(fundata$temp_c))
```

**Exercise:** Remove the row with the `NA` as a value for `temp_c`

"NA" is its own type in R.

```{r}
is.na(2)
is.na(NA)
is.na(c(1, NA, 3))

fish <- c("gag grouper", NA, "red drum", NA)
fish

# Filter NAs in a character vector
fish[!is.na(fish)]
```

```{r}
# Remember we can subset the rows in a data.frame like:
fundata[c(1, 2, 3),] # First, second, third rows
#or
fundata[which(fundata$site == "A"),] # Just the rows with site == "A"

# Write an expression using `is.na` to subset fundata and save the result
# Your code here:
# e.g. fundata$site <- 
```

##### Check for duplicate values

Since have a hierarchical design, we might want to check that all the data we expect are present.
If we know we have 6 sites with 6 plots at each site and ten samples, we can check this assumption a few ways:
  
```{r}
nrow(fundata) == 6 * 6 * 10
```

**Exercise:** Use `dplyr` with `group_by` and `summarize` to find which `site` or `plot` has the wrong number of observations

```{r}
# Put your solution here
```

The functions `unique` and `duplicated` great ways to find duplicates in a vector of values.
For example, we can create a vector with a duplicate value in it:

```{r}
some_letters <- c("A", "F", "X", "S", "X")
some_letters
length(unique(some_letters)) == length(some_letters)
duplicated(some_letters)
some_letters[!duplicated(some_letters)]
```

See how `nrow` and `unique` can tell us *if* there are duplicates and `duplicated` can tell us *which* values are duplicates?
  
**Exercise:** Remove the duplicate value with duplicated from the following data.frame:
  
  ```{r}
mydf <- data.frame(fish = c("Redfish", "Gag Grouper", "Striped Bass", "Redfish"),
                   size = runif(4, 50, 100),
                   stringsAsFactors = FALSE)
# Your code here
```

#### Complete analysis script:

```{r}
library(dplyr)

# Import
fundata <- read.csv("./data/fundata.csv", stringsAsFactors = FALSE)

# Munge
fundata <- fundata %>% 
  separate(siteplot, c("site", "plot"), sep = "\\.")
fundata$date <- as.Date(fundata$date, "%m-%d-%Y")

# Check
any(is.na(fundata$site))
any(fundata$temp_c < 0)
any(fundata$temp_c > 100)

# Fix temp_c column:
fundata <- fundata[which(!is.na(fundata$temp_c)),] # Remove the NA
fundata <- fundata[fundata$temp_c != -9999,]
fundata <- fundata[fundata$temp_c != 180,]

# Analyze
fundata %>% 
  group_by(site) %>% 
  summarise(meantemp = mean(temp_c))
```

#### more `tidyr` package

We already saw how useful `separate` can be. 
`tidyr` provides two other functions I use all the time: `gather` and `spread`.

##### `gather`

`gather` takes our data that is *wide* (multiple observations on each row) and puts it into a *tall* form where each row is an observation.

```{r}
# Make some 'wide' data first
fishdf <- data.frame(fish = c("Redfish", "Gag Grouper", "Striped Bass"),
                     weight = runif(3, 2, 10),
                     age = runif(3, 5, 80))
fishdf

# Gather it
fishdf_g <- gather(fishdf, variable, value, -fish)
fishdf_g
```

Why gather? Analysis often needs us to reshape the data in this way:
  
```{r}
library(dplyr)

fishdf_g %>% 
  group_by(variable) %>% 
  summarize(meanvalue = mean(value))
```

Also, ggplot2 needs this form:
  
```{r}
library(ggplot2)
ggplot(fishdf_g, aes(variable, value)) + 
  geom_boxplot()
```

##### `spread`

`spread` does the opposite of `gather`:
  
```{r}
fishdf_g %>% spread(variable, value)
```

Why spread? Usually modeling requires this "wide" format:
  
```{r}
lm(weight ~ age, data = fishdf)
```

So with the combination of `gather` and `spread`, we can easily switch between analysis and modeling.

#### The `assertr` package approach

> The assertr package supplies a suite of functions designed to verify assumptions about data early in an analysis pipeline so that data errors are spotted early and can be addressed quickly.

- Website: https://github.com/ropensci/assertr

The basic idea is that we should check qualities of our dataset prior to analysis and that we can actually make the analysis not run if certain assertions are not met.

```{r, eval=FALSE}
library(assertr)

mtcars %>% verify(TRUE) %>% nrow(.)
mtcars %>% verify(FALSE) %>% nrow(.)

# Only plot mpg ~ wt if the columns exist and all mpg are > 0
mtcars %>% 
  verify(has_all_names("mpg", "wt")) %>% 
  verify(mpg > 0) %>% 
  ggplot(aes(wt, mpg)) + geom_point()
```

Let's walk through the introduction taken from the [README](https://github.com/ropensci/assertr):

Let's work with the `mtcars` dataset.
We don't know who created it and, in order to do an analysis with it, we might want to check a few assumptions:

- that it has the columns "mpg", "vs", and "am"
- that the dataset contains more than 10 observations
- that the column for 'miles per gallon' (mpg) is a positive number
- that the column for ‘miles per gallon’ (mpg) does not contain a datum that is outside 4 standard - deviations from its mean, and
- that the am and vs columns (automatic/manual and v/straight engine, respectively) contain 0s and 1s only
- each row contains at most 2 NAs
- each row is unique jointly between the "mpg", "am", and "wt" columns
- each row's mahalanobis distance is within 10 median absolute deviations of all the distances (for outlier detection)

This could be written (in order) using assertr like this:
  
```{r}
library(dplyr)
library(assertr)

mtcars %>%
  verify(has_all_names("mpg", "vs", "am", "wt")) %>%
  verify(nrow(.) > 10) %>%
  verify(mpg > 0) %>%
  insist(within_n_sds(4), mpg) %>%
  assert(in_set(0,1), am, vs) %>%
  assert_rows(num_row_NAs, within_bounds(0,2), everything()) %>%
  assert_rows(col_concat, is_uniq, mpg, am, wt) %>%
  insist_rows(maha_dist, within_n_mads(10), everything()) %>%
  group_by(cyl) %>%
  summarise(avg.mpg=mean(mpg))
```


If you look closely, the last two lines in the above chunk are our analysis, but everything before it are assertions.

**Exercise:** Let's do an assertr analysis pipeline with the starwars dataset

```{r}
library(dplyr)
head(starwars)
```

What are some things we might want to analyze?

- Average height/mass by species/gender/homeworld?

What are some things we might like to assert *before* we do that analysis?

- birth_year/height/mass is a positive real number
- no Droids should have Gender (maybe)?

### Summary

- There are a number of ways to munge and qc data with base R
- The `tidyr` package provides a few very useful functions that do things not easily done in base R
- The `assertr` package shows a new methodology where the assertions are built into the analysis pipeline

### Extra things to do:

#### `starwars` dataset

**Exercise:** Check if any droids have hair or gender

```{r}
library(dplyr)
head(starwars)
# Your code here
```

**Exercise:** Make sure every character has been in at least one film

```{r}
# Your code here
```

### Misc

This chunk generates the fundata.csv file we used in this lesson

```{r}
n <- 6
reps <- 10
fundata <- data.frame(site = rep(rep(c("A", "B", "C", "D", "E", "F"), n), reps),
plot = rep(rep(c(1:6), n), reps),
date = rep(c("03-21-2017", "04-21-2017", "05-21-2017", "06-21-2017", "07-21-2017", "08-21-2017"), reps),
temp_c = round(rnorm(n * n * reps, 30, 5), 2),
stringsAsFactors = FALSE)

# omit 5 rows at random
fundata <- fundata[-sample(1:nrow(fundata), 5, replace = FALSE),]

# replace 5 values with -999
fundata[sample(1:nrow(fundata), 5, replace = FALSE), "temp_c"] <- -9999

# replace 1 value with a way-too-high value
fundata[sample(1:nrow(fundata), 1), "temp_c"] <- 180

# mush site and plot together into one column
fundata$siteplot <- paste(fundata$site, fundata$plot, sep = ".")

# clean it up and save it
fundata <- fundata[,c("siteplot", "date", "temp_c")]
write.csv(fundata, row.names = FALSE, file = "fundata.csv")
```



## Regular Expressions in R

Regular expressions are a fantastic tool for filtering and even extracting information out of strings of characters such as site codes, titles, or even entire documents. 
Regular expressions follow a custom syntax that we'll need to learn but they are worth learning because:

- Regular expressions can do things other methods cannot
- Regular expressions can be used with many other languages and tools so it's a learn-once, use-everywhere kind of tool

But they're somethig that you only need to learn a bit of to get a lot of value out of them.
I often use fairly simple regular expressions, like the ones we used on the command line,

```bash
ls *.Rmd
```

### Learning Outcomes
  
Students should:
  
- Understand when regular expressions are appropriate
- Have an introductory-level awareness of regular expression syntax
- Have some experience executing and working with regular expressions in R

### Lesson

Earlier this week, we used some simple regular expression on the command line (terminal).
The same type of operations we used on the command line work in R:

```{r}
getwd() # Like pwd()
dir() # Like `ls()`

library(stringr)
str_view_all(dir(), ".*Rmd")
str_view_all(dir(), ".*html")
```

Let's start off with a simple example of where simpler methods won't work and see how regular expressions can be used to get what we need done.
Let's say we just received some data we need to analyze and we find this:

```{r}
site_data <- read.csv("./data/site_data.csv", stringsAsFactors = FALSE)
site_data
```

It looks like the author of the dataset mixed the year of measurements, site code (e.g., A, CCCC, etc.), and some sub-site code (e.g., 1, 2, 3, etc.) into a single column.
If we wanted to, for example, calculate mean temperature by site, we'd need to split these up somehow into separate columns.
How could we go about this?
We could start with `substr` which lets us slice a string by its indices:

```{r}
substr(site_data$x, 1, 4)
substr(site_data$x, 5, 16)
```

But we'd quickly find that, because the number of characters in the site code varies from one to four, we can't extract just the site code. 
These are the types of problems where regular expressions come in handy.

Before we start, we're going to use the `str_view_all` function from the `stringr` package which shows a nice display of the result of executing a regular expression against our strings.
In real use, we would use another function to actually get and work with the result.

```{r}
library(stringr)
str_view_all(site_data$x, "[a-z ]+")
```

The expression we used above, `[a-z ]+`, is equivalent to asking for the first consecutive run of the letters a-z or " " (a space) in the entire string of characters.
This is the type of problem regular expression were created for!


### Overview of Regular Expressions

Regular expressions can match things literally, e.g.,

```{r}
str_detect("grouper", "striper")
str_detect("grouper", "grouper")
```

but they also support a large set of special characters:

- `.`: Match any character

```{r}
fish <- c("grouper", "striper", "sheepshead")
str_view_all(fish, ".p")
```

If you actually want to match a period and not any character, you have to do what's called escaping:

```{r}
fish <- c("stripers", "striper.", "grouper")
str_view_all(fish, "striper\\.")
```

See how that regular expression only matched the striper with the period at the end and not the string stripers?

- `[]`: Match any character in this set

```{r}
fish <- c("grouper", "striper", "sheepshead")
str_view_all(fish, "[aeiou]")
```


- `[^]`: Match any character *not* in this set

```{r}
fish <- c("grouper", "striper", "sheepshead")
str_view_all(fish, "[^aeiou]")
```

- `\s` & `\S`: Match any whitespace (e.g., ` `, `\t`)

```{r}
fish <- c("gag grouper", "striper", "red drum")
str_view_all(fish, "\\s") # Note the double \\ before the s. This is an R-specific thing.
                           # many of our special characters must be preceded by a \\
str_view_all(fish, "\\S")
```
Note that the lower case version `\\s` selects any whitespace characters, whereas the uppercase version `\\S` selects all non-whitespace characters.  The next pattern is analogous for digits:

- `\d` & `\D`: Match any digit, equivalent to `[0-9]`

```{r}
fish <- c("striper1", "red drum2", "tarpon123")
str_view_all(fish, "\\d")
```

- `\w` & `\W`: Match any word character, equivalent to `[A-Za-z0-9_]`

```{r}
fish <- c("striper1", "red drum2", "tarpon123")
str_view_all(fish, "\\w")
```

We can also specify how many of a particular character or class of character to match:

- `?` Optionality / 0 or 1

Say we want to get just the phone numbers out of this vector but we notice that the phone numbers take on some different formats:

```{r}
phone_numbers <- c("219 733 8965", "apple", "329-293-8753 ", "123banana", "595.794.7569", "3872876718")
str_view_all(phone_numbers, "\\d\\d\\d[ \\.-]?\\d\\d\\d[ \\.-]?\\d\\d\\d\\d")
```

The above regular expression matches the number parts of the phone numbers, which can be separated by zero or one space (` `), `.`, or `-`.

- `+` 1 -> infinity

We can use the `+` expression to find words with one or more vowels:

```{r}
fish <- c("gag grouper", "striper", "red drum", "cobia", "sheepshead")
str_view_all(fish, "[aeiuo]+")
```

- `*` 0 -> infinity

and the `*` is _zero or more_`.

```{r}
numbers <- c("0.2", "123.1", "547")
str_view_all(numbers, "\\d*\\.?\\d*")

# Regular expressions are greedy
letters <- "abcdefghijkc"
str_view_all(letters, "a.*c") # Greedy
str_view_all(letters, "a.*?c") # Lazy
```

- `()`: Grouping

One of the most powerful parts of regular expressions is grouping.
Grouping allows us to split up our matched expressions and do more work with them.
For example, we can create match the city and state in a set of addresses, splitting it into components:

```{r}
addresses <- c("Santa Barbara, CA", "Seattle, WA", "New York, NY")
str_view_all(addresses, "([\\w\\s]+), (\\w+)")
str_match_all(addresses, "([\\w\\s]+), (\\w+)")
```

Once we use groups, `()`, we can also use back references to work with the result.
Back references are \\ and a number, where \\1 is the first thing in (), \\2 is the second thing in (), and so on.

```{r}
str_replace(addresses, "([\\w\\s]+), (\\w+)", "City: \\1, State: \\2")
```

- `^` & `$`

It can also be really useful to make a say something like "strings that start with a capital letter" or "strings that end with a period":

```{r}
possible_sentences <- c(
  "This might be a sentence.",
  "So. Might. this",
  "but this could maybe not be?",
  "Am I a sentence?",
  "maybe not",
  "Regular expressions are useful!"
)
# ^ specifies the start so ^[A-z] means "starts with a capital letter""
str_detect(possible_sentences, "^[A-Z]")
possible_sentences[str_detect(possible_sentences, "^[A-Z]")]

# We can also do "ends with a period"
str_detect(possible_sentences, "\\.$")
possible_sentences[str_detect(possible_sentences, "\\.$")]

# We can put them together:
str_detect(possible_sentences, "^[A-Z].*[\\.\\?!]$")
possible_sentences[str_detect(possible_sentences, "^[A-Z].*[\\.\\?!]$")]
```


### Finish out our example together

Now that we've gone over some basics of regular expressions, let's finish our example by splitting the various components of column `x` into a `year`, `site`, and `sub_site` column:

```{r}
site_data

# I'll show you how to extract the year part
site_data$year <- str_extract(site_data$x, "\\d{4}")

# You do the rest
site_data$site <- str_extract(site_data$x, "") # <- Fill this in between the ""
site_data$plot <- str_extract(site_data$x, "") # <- Fill this in between the ""
```

### Common R functions that use regular expressions

- Base R
  - `grep`
  - `gsub`
  - `strsplit`
- `stringr` package
  - `string_detect`
  - `string_match`
  - `string_replace`
  - `string_split`

#### Another example

Data often come to us in strange forms and, before we can even begin analyzing the data, we have to do a lot of work to sanitize what we've been given.
An example, which I just got the other week were temporal data with dates formatted like this:

```{r}
dates <- c("1July17",
           "02July2017",
           "3July17",
           "4July17")
```

and so on like that.
Do you see how the day of the month and year are represented in different ways through the series?
If we want to convert these strings into `Date` objects for further analysis, we'll to do some pre-cleaning before we can do that conversion.
Regular expressions work great here.

```{r}
str_match_all(dates, "(\\d{1,2})([A-Za-z]+)(\\d{2,4})")
```

That above regular expression was complex. Let's break it down into its main parts.
Below, I've re-formatted the data and the regular expression a bit so we can see what's going on.

```
|---------------|---------------| ---------|
| 1             | July          | 17       |
| 02            | July          | 2017     |
| 3             | July          | 17       |
| 4             | July          | 17       |
|---------------|---------------| ---------|
| \\d{1,2}      | [A-Za-z]+     | \\d{2,4} |
|---------------|---------------| ---------|

```

### Summary

- Regular expressions are a crucial tool in the data analysis toolbox
- Regular expressions help us solve problems we may not be otherwise able to solve
- Regular expressions are supported in many functions in R

### More

- Have the group figure out you can put * onto [].
- Have the group str_split on fixed chars and a regex

### Resources

- [https://regex101.com/](https://regex101.com/) (This is the site I usually use)
- [https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference](https://docs.microsoft.com/en-us/dotnet/standard/base-types/regular-expression-language-quick-reference)
- [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions)

### Appendicies

Here's the code I used to generate the fake site_data `data.frame` above.

```{r, eval=FALSE, echo=FALSE, include=FALSE}
# Fake data generation code
site_data <- data.frame(year = rep(seq(2000, 2009), 4))
site_data$site <- sample(c("galveston bay", "choctawhatchee bay", "aransas bay", "copany bay"), nrow(site_data), replace = TRUE)
site_data$subsite <- sample(c(1,2,3,4), nrow(site_data), replace=TRUE)
site_data$temp_c <- runif(nrow(site_data), 0, 100)
site_data$site_code <- paste(site_data$year, site_data$site, site_data$subsite, sep="-")
site_data <- site_data[,c("site_code", "temp_c")]
names(site_data) <- c("x", "temp_c")
write.csv(site_data, file = "site_data.csv", row.names = FALSE)
```


## Introduction to Text Mining


The Goal of this session is to learn how to mine PDFs to get information out of them. Text mining encompasses a vast field of theoretical approaches and methods with one thing in common: text as input information (Feiner et al, 2008) 


### Which R packages are available?

Looking at the Natural Language Processing (NLP) CRAN view, you will realize there are a lot of different packages to accomplish this complex task : <https://cran.r-project.org/web/views/NaturalLanguageProcessing.html>.

Here are some important packages:

- `tm`: provides a framework and the algorithmic background for mining text
- `quanteda`: A fast and flexible framework for the management, processing, and quantitative analysis of textual data in R. It has very nice features, among which include finding specific words and their context in the text
- `tidytext`: provides means for text mining for word processing and sentiment analysis using dplyr, ggplot2, and other tidy tools

***=> In this quick introduction we are going to use `quanteda`***


### Analyzing peer-reviewed journal articles about BP Deep Horizon's oil spill

First, let us load the necessary packages

```{r,warning=F,message=F}
library("readtext")
library("quanteda")
```

#### 1. Import the PDFs into R

```{r}
# set path to the PDF (here on Aurora)
pdf_path <- "./data-liberation/oil_spill_pdfs"

# List the PDFs about the BP oil spill
pdfs <- list.files(path = pdf_path, pattern = 'pdf$',  full.names = TRUE) 

# Import the PDFs into R
spill_texts <- readtext(pdfs, 
                        docvarsfrom = "filenames", 
                        sep = "_", 
                        docvarnames = c("First_author", "Year"))
```

#### 2. Create the Corpus object needed for the text analysis

```{r}
# Transform the journal articles into a corpus object
spill_corpus  <- corpus(spill_texts)

# Some stats about the journal articles
tokenInfo <- summary(spill_corpus)
```

##### Add metadata to the Corpus object

For example we can add the information that these texts are written in English.
```{r}
# add metadata to files, in this case that they are written in english
metadoc(spill_corpus, 'language') <- "english" 

# visualize corpus structure and contents, now with added metadata
summary(spill_corpus, showmeta = T)
```

##### Subset coprus

Do you want only articles before 2017?

```{r}
summary(corpus_subset(spill_corpus, Year < 2017))
```

##### Search for words with context: 4 words on each side of the keyword

```{r, results="hide"}
kwic(spill_corpus, "dispersant", 4)
```


#### 3. Build a Document-Feature Matrix (DFM) 

More information about DFM can be found on Quanteda's vignette: http://quanteda.io/articles/quickstart.html. In a nutshell, additional rules can be applied on top of the tokenization process, such as ignoring certain words, punctuation, case, ...

```{r}
# construct the DFM, which is the base object to further analyze the journal articles
spills_DFM <- dfm(spill_corpus, tolower = TRUE, stem = FALSE, 
                  remove = c("et", "al", "fig", "table", "ml", "http",
                             stopwords("SMART")),
                  remove_punct = TRUE, remove_numbers = TRUE)

# returns the top 20 frequent words
topfeatures(spills_DFM, 20) 
```

Note: You can check what words are listed by default in stopwords:
```{r}
head(stopwords("english"), 20)
```

#### 4. Extract information from a Document-Feature Matrix (DFM) 

##### Word cloud

Quickly visualize the most frequent words:

```{r}
# set the seed for wordcloud
set.seed(1)

# plots wordcloud
textplot_wordcloud(spills_DFM, min.freq = 60, random.order=F, 
                   rot.per = .10,  
                   colors = RColorBrewer::brewer.pal(8,'Dark2')) 
```


##### Grouping documents by metadata

Here we are grouping the documents by year of publication:

```{r}
spills_DFM_yearly <- dfm(spill_corpus, groups = "Year", tolower = TRUE, stem = TRUE, 
                  remove = c("et", "al", "fig", "table", "ml", "http",
                             stopwords("SMART")),
                  remove_punct = TRUE, remove_numbers = TRUE)

# Sort by year and show the top 20 most frequent words
dfm_sort(spills_DFM_yearly)[,1:20]
```

##### Searching for concepts using sets of keywords

One very powerful feature of `quanteda` is to allow to group keywords by dictionary to mine texts. 

```{r}
myDict <- dictionary(list(pollution = c("oil", "oiled", "crude", "petroleum", "pahs", "pah", "tph", "benzo", "hydrocarbons", "pollution"),
                          measurement = c("data", "sample", "samples", "sampling", "study")))
```

```{r}
spills_DFM <- dfm(spill_corpus, dictionary = myDict)

spills_DFM
```

***The above text manipulations are the necessary steps to enable more advanced text analaysis, such as topical modeling and similarities between texts.***


### References and sources

- Book on text mining in R: http://tidytextmining.com/
- `tm` package: https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf
- `quanteda` package:
    - Overview: https://github.com/kbenoit/quanteda
    - Getting started: http://quanteda.io/articles/quickstart.html
- Munzert, Simon. Automated Data Collection with R: A Practical Guide to Web Scraping and Text Mining. Chichester, West Sussex, United Kingdom: John Wiley & Sons Inc, 2015.
- [Text Mining with R](http://tidytextmining.com): A Tidy Approach, by Julia Silge and David Robinson.




